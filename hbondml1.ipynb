{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subjective-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acid</th>\n",
       "      <th>Base</th>\n",
       "      <th>Agroup</th>\n",
       "      <th>Aeneg</th>\n",
       "      <th>Asize</th>\n",
       "      <th>Beneg</th>\n",
       "      <th>Bsize</th>\n",
       "      <th>Δν</th>\n",
       "      <th>-ΔH (kJ mol-1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CCl3)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CH2Cl)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CHCl2)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acetic acid</td>\n",
       "      <td>Pyridine</td>\n",
       "      <td>COOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acetic acid</td>\n",
       "      <td>Dimethyl sulfoxide</td>\n",
       "      <td>COOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Acid                Base Agroup  Aeneg  Asize  Beneg  Bsize  \\\n",
       "0   (CCl3)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "1  (CH2Cl)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "2  (CHCl2)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "3   Acetic acid            Pyridine   COOH   3.41    150   2.90  166.0   \n",
       "4   Acetic acid  Dimethyl sulfoxide   COOH   3.41    150   3.41  150.0   \n",
       "\n",
       "       Δν  -ΔH (kJ mol-1)  \n",
       "0   211.0            22.6  \n",
       "1   102.0            14.2  \n",
       "2   174.0            19.8  \n",
       "3  1000.0            40.0  \n",
       "4   840.0            33.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "DATA_FILENAME = \"hbonddata.xlsx\"\n",
    "dataframe_raw = pd.read_excel(DATA_FILENAME,sheet_name='data')\n",
    "dataframe_raw = dataframe_raw.filter(items=['Acid', 'Base', 'Agroup', 'Aeneg', 'Asize', 'Beneg', 'Bsize', 'Δν', '-ΔH (kJ mol-1)'])\n",
    "# Drop the rows with NANs. \n",
    "dataframe_raw = dataframe_raw.dropna(axis=0, how = 'any')\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powerful-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acid</th>\n",
       "      <th>Base</th>\n",
       "      <th>Agroup</th>\n",
       "      <th>Aeneg</th>\n",
       "      <th>Asize</th>\n",
       "      <th>Beneg</th>\n",
       "      <th>Bsize</th>\n",
       "      <th>Δν</th>\n",
       "      <th>ΔH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CCl3)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CH2Cl)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CHCl2)2CHOH</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acetic acid</td>\n",
       "      <td>Pyridine</td>\n",
       "      <td>COOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acetic acid</td>\n",
       "      <td>Dimethyl sulfoxide</td>\n",
       "      <td>COOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Acid                Base Agroup  Aeneg  Asize  Beneg  Bsize  \\\n",
       "0   (CCl3)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "1  (CH2Cl)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "2  (CHCl2)2CHOH        Acetonitrile    COH   3.41    150   2.90  166.0   \n",
       "3   Acetic acid            Pyridine   COOH   3.41    150   2.90  166.0   \n",
       "4   Acetic acid  Dimethyl sulfoxide   COOH   3.41    150   3.41  150.0   \n",
       "\n",
       "       Δν    ΔH  \n",
       "0   211.0  22.6  \n",
       "1   102.0  14.2  \n",
       "2   174.0  19.8  \n",
       "3  1000.0  40.0  \n",
       "4   840.0  33.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns\n",
    "dataframe_raw.rename(columns={'-ΔH (kJ mol-1)': 'ΔH'}, inplace =True)\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "freelance-buddy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aeneg</th>\n",
       "      <th>Asize</th>\n",
       "      <th>Beneg</th>\n",
       "      <th>Bsize</th>\n",
       "      <th>Δν</th>\n",
       "      <th>ΔH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>318.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.238428</td>\n",
       "      <td>155.789308</td>\n",
       "      <td>3.172390</td>\n",
       "      <td>157.122642</td>\n",
       "      <td>317.967614</td>\n",
       "      <td>19.464534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.325150</td>\n",
       "      <td>10.909563</td>\n",
       "      <td>0.303832</td>\n",
       "      <td>8.586949</td>\n",
       "      <td>350.151747</td>\n",
       "      <td>11.840464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.480000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.410000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>109.192487</td>\n",
       "      <td>10.632760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.410000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>218.741953</td>\n",
       "      <td>17.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.410000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>364.500000</td>\n",
       "      <td>25.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.410000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Aeneg       Asize       Beneg       Bsize           Δν          ΔH\n",
       "count  318.000000  318.000000  318.000000  318.000000   318.000000  318.000000\n",
       "mean     3.238428  155.789308    3.172390  157.122642   317.967614   19.464534\n",
       "std      0.325150   10.909563    0.303832    8.586949   350.151747   11.840464\n",
       "min      2.480000  150.000000    2.040000  150.000000     1.000000    0.700000\n",
       "25%      3.410000  150.000000    2.900000  150.000000   109.192487   10.632760\n",
       "50%      3.410000  150.000000    3.410000  150.000000   218.741953   17.600000\n",
       "75%      3.410000  150.000000    3.410000  166.000000   364.500000   25.104000\n",
       "max      3.410000  189.000000    3.410000  177.000000  2300.000000   67.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the info of the data\n",
    "dataframe_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subsequent-tracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agroup</th>\n",
       "      <th>Aeneg</th>\n",
       "      <th>Asize</th>\n",
       "      <th>Beneg</th>\n",
       "      <th>Bsize</th>\n",
       "      <th>Δν</th>\n",
       "      <th>ΔH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.160000</td>\n",
       "      <td>12.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>280.720000</td>\n",
       "      <td>12.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>HOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>480.240000</td>\n",
       "      <td>32.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>NH</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>498.800000</td>\n",
       "      <td>24.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>PhOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.955647</td>\n",
       "      <td>17.355650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>421.080000</td>\n",
       "      <td>26.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>PhOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>951.200000</td>\n",
       "      <td>37.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>COH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>2.90</td>\n",
       "      <td>166.0</td>\n",
       "      <td>350.260170</td>\n",
       "      <td>17.024278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>PhOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>392.080000</td>\n",
       "      <td>33.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>PhOH</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150</td>\n",
       "      <td>3.41</td>\n",
       "      <td>150.0</td>\n",
       "      <td>444.280000</td>\n",
       "      <td>28.413000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Agroup  Aeneg  Asize  Beneg  Bsize          Δν         ΔH\n",
       "266    COH   3.41    150   3.41  150.0  117.160000  12.177000\n",
       "79     COH   3.41    150   2.90  166.0  280.720000  12.870000\n",
       "324    HOH   3.41    150   2.90  166.0  480.240000  32.076000\n",
       "241     NH   2.90    166   2.90  166.0  498.800000  24.453000\n",
       "156   PhOH   3.41    150   3.41  150.0  138.955647  17.355650\n",
       "..     ...    ...    ...    ...    ...         ...        ...\n",
       "171    COH   3.41    150   2.90  166.0  421.080000  26.532000\n",
       "172   PhOH   3.41    150   2.90  166.0  951.200000  37.719000\n",
       "92     COH   3.41    150   2.90  166.0  350.260170  17.024278\n",
       "67    PhOH   3.41    150   3.41  150.0  392.080000  33.165000\n",
       "182   PhOH   3.41    150   3.41  150.0  444.280000  28.413000\n",
       "\n",
       "[270 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_name = \"Stcike sarpherise Lei\" # at least 5 characters\n",
    "def customize_dataset(dataframe_raw, rand_str):\n",
    "    dataframe = dataframe_raw.copy(deep=True)\n",
    "    # drop some rows\n",
    "    dataframe = dataframe.sample(int(0.85*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
    "    # scale input\n",
    "    dataframe.Δν = dataframe.Δν * ord(rand_str[1])/100.\n",
    "    # scale target\n",
    "    dataframe.ΔH = dataframe.ΔH * ord(rand_str[2])/100.\n",
    "    # drop column\n",
    "    if ord(rand_str[3]) % 2 == 1:\n",
    "        dataframe = dataframe.drop(['Acid','Base'], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "dataframe = customize_dataset(dataframe_raw, your_name)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "multiple-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['Aeneg', 'Asize', 'Beneg', 'Bsize', 'Δν']\n",
    "categorical_cols = ['Agroup']\n",
    "output_cols = ['ΔH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cathedral-coordination",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  3.41      , 150.        ,   3.41      , 150.        ,\n",
       "         117.16      ],\n",
       "        [  3.41      , 150.        ,   2.9       , 166.        ,\n",
       "         280.72      ],\n",
       "        [  3.41      , 150.        ,   2.9       , 166.        ,\n",
       "         480.24      ],\n",
       "        ...,\n",
       "        [  3.41      , 150.        ,   2.9       , 166.        ,\n",
       "         350.26016967],\n",
       "        [  3.41      , 150.        ,   3.41      , 150.        ,\n",
       "         392.08      ],\n",
       "        [  3.41      , 150.        ,   3.41      , 150.        ,\n",
       "         444.28      ]]),\n",
       " array([[12.177    ],\n",
       "        [12.87     ],\n",
       "        [32.076    ],\n",
       "        [24.453    ],\n",
       "        [17.3556504],\n",
       "        [ 4.158    ],\n",
       "        [13.3791768],\n",
       "        [40.593168 ],\n",
       "        [ 6.633    ],\n",
       "        [14.49756  ],\n",
       "        [39.35052  ],\n",
       "        [ 7.92     ],\n",
       "        [12.7992744],\n",
       "        [45.149544 ],\n",
       "        [12.1779504],\n",
       "        [38.107872 ],\n",
       "        [43.906896 ],\n",
       "        [31.0662   ],\n",
       "        [14.949    ],\n",
       "        [15.4088352],\n",
       "        [ 8.91     ],\n",
       "        [21.0421728],\n",
       "        [19.3853088],\n",
       "        [19.468152 ],\n",
       "        [13.0063824],\n",
       "        [30.69     ],\n",
       "        [ 9.108    ],\n",
       "        [35.64     ],\n",
       "        [22.374    ],\n",
       "        [24.849    ],\n",
       "        [32.769    ],\n",
       "        [15.741    ],\n",
       "        [ 7.029    ],\n",
       "        [ 4.059    ],\n",
       "        [11.0595672],\n",
       "        [ 2.079    ],\n",
       "        [13.2134904],\n",
       "        [15.6159432],\n",
       "        [ 9.108    ],\n",
       "        [33.13728  ],\n",
       "        [19.305    ],\n",
       "        [23.76     ],\n",
       "        [15.2431488],\n",
       "        [12.4679016],\n",
       "        [20.295    ],\n",
       "        [ 7.524    ],\n",
       "        [ 9.941184 ],\n",
       "        [ 6.237    ],\n",
       "        [ 4.653    ],\n",
       "        [10.791    ],\n",
       "        [23.859    ],\n",
       "        [21.539232 ],\n",
       "        [37.6108128],\n",
       "        [24.85296  ],\n",
       "        [27.72     ],\n",
       "        [ 2.97     ],\n",
       "        [20.691    ],\n",
       "        [10.8938808],\n",
       "        [34.794144 ],\n",
       "        [11.598048 ],\n",
       "        [15.642    ],\n",
       "        [20.295    ],\n",
       "        [16.982856 ],\n",
       "        [ 6.336    ],\n",
       "        [23.166    ],\n",
       "        [ 7.524    ],\n",
       "        [21.978    ],\n",
       "        [ 1.782    ],\n",
       "        [18.216    ],\n",
       "        [33.13728  ],\n",
       "        [12.078    ],\n",
       "        [25.245    ],\n",
       "        [19.998    ],\n",
       "        [19.701    ],\n",
       "        [28.6223256],\n",
       "        [ 4.653    ],\n",
       "        [22.176    ],\n",
       "        [17.424    ],\n",
       "        [ 6.633    ],\n",
       "        [14.85     ],\n",
       "        [35.4568896],\n",
       "        [11.598048 ],\n",
       "        [40.59     ],\n",
       "        [19.0953576],\n",
       "        [ 8.91     ],\n",
       "        [21.953448 ],\n",
       "        [18.315    ],\n",
       "        [21.78     ],\n",
       "        [24.438744 ],\n",
       "        [ 4.752    ],\n",
       "        [15.345    ],\n",
       "        [20.394    ],\n",
       "        [39.303    ],\n",
       "        [15.345    ],\n",
       "        [33.561    ],\n",
       "        [25.344    ],\n",
       "        [32.868    ],\n",
       "        [15.543    ],\n",
       "        [18.225504 ],\n",
       "        [20.1723192],\n",
       "        [ 3.168    ],\n",
       "        [13.266    ],\n",
       "        [13.2134904],\n",
       "        [16.533    ],\n",
       "        [ 8.118    ],\n",
       "        [25.245    ],\n",
       "        [19.882368 ],\n",
       "        [31.086    ],\n",
       "        [27.338256 ],\n",
       "        [ 7.821    ],\n",
       "        [19.602    ],\n",
       "        [15.741    ],\n",
       "        [17.127    ],\n",
       "        [ 7.425    ],\n",
       "        [20.295    ],\n",
       "        [35.64     ],\n",
       "        [ 9.504    ],\n",
       "        [17.82     ],\n",
       "        [36.432    ],\n",
       "        [65.34     ],\n",
       "        [43.56     ],\n",
       "        [14.949    ],\n",
       "        [20.4208488],\n",
       "        [ 4.554    ],\n",
       "        [22.275    ],\n",
       "        [22.9061448],\n",
       "        [45.56376  ],\n",
       "        [ 9.9      ],\n",
       "        [22.374    ],\n",
       "        [18.216    ],\n",
       "        [ 8.217    ],\n",
       "        [16.137    ],\n",
       "        [14.704668 ],\n",
       "        [ 4.95     ],\n",
       "        [16.236    ],\n",
       "        [29.898    ],\n",
       "        [44.55     ],\n",
       "        [41.58     ],\n",
       "        [12.87     ],\n",
       "        [43.906896 ],\n",
       "        [21.74634  ],\n",
       "        [26.92404  ],\n",
       "        [15.741    ],\n",
       "        [12.177    ],\n",
       "        [30.03066  ],\n",
       "        [13.266    ],\n",
       "        [10.989    ],\n",
       "        [16.0715808],\n",
       "        [20.1308976],\n",
       "        [30.651984 ],\n",
       "        [30.69     ],\n",
       "        [16.929    ],\n",
       "        [ 5.841    ],\n",
       "        [29.7      ],\n",
       "        [57.42     ],\n",
       "        [19.008    ],\n",
       "        [29.403    ],\n",
       "        [22.2433992],\n",
       "        [17.424    ],\n",
       "        [ 7.92     ],\n",
       "        [14.3318736],\n",
       "        [30.393    ],\n",
       "        [ 8.118    ],\n",
       "        [17.5213368],\n",
       "        [35.046    ],\n",
       "        [19.468152 ],\n",
       "        [ 7.623    ],\n",
       "        [33.165    ],\n",
       "        [17.424    ],\n",
       "        [ 6.039    ],\n",
       "        [ 3.564    ],\n",
       "        [ 9.504    ],\n",
       "        [36.63     ],\n",
       "        [38.61     ],\n",
       "        [17.82     ],\n",
       "        [16.2372672],\n",
       "        [35.64     ],\n",
       "        [ 9.504    ],\n",
       "        [36.451008 ],\n",
       "        [28.580904 ],\n",
       "        [17.1485424],\n",
       "        [ 9.108    ],\n",
       "        [ 6.633    ],\n",
       "        [14.256    ],\n",
       "        [22.78188  ],\n",
       "        [21.9120264],\n",
       "        [10.395    ],\n",
       "        [17.397072 ],\n",
       "        [24.75     ],\n",
       "        [ 9.9      ],\n",
       "        [11.187    ],\n",
       "        [18.513    ],\n",
       "        [ 7.029    ],\n",
       "        [21.953448 ],\n",
       "        [14.058    ],\n",
       "        [11.187    ],\n",
       "        [41.4216   ],\n",
       "        [ 9.9      ],\n",
       "        [20.7108   ],\n",
       "        [18.63972  ],\n",
       "        [19.206    ],\n",
       "        [10.1897136],\n",
       "        [ 1.584    ],\n",
       "        [20.4208488],\n",
       "        [17.424    ],\n",
       "        [15.9058944],\n",
       "        [24.453    ],\n",
       "        [18.612    ],\n",
       "        [ 8.217    ],\n",
       "        [12.0951072],\n",
       "        [ 7.9529472],\n",
       "        [20.7108   ],\n",
       "        [28.116    ],\n",
       "        [28.99512  ],\n",
       "        [22.367664 ],\n",
       "        [ 0.693    ],\n",
       "        [43.56     ],\n",
       "        [12.7992744],\n",
       "        [20.4208488],\n",
       "        [ 6.237    ],\n",
       "        [12.1779504],\n",
       "        [10.89     ],\n",
       "        [22.572    ],\n",
       "        [35.64     ],\n",
       "        [ 2.07108  ],\n",
       "        [ 4.95     ],\n",
       "        [13.266    ],\n",
       "        [13.5034416],\n",
       "        [39.35052  ],\n",
       "        [18.612    ],\n",
       "        [ 7.92     ],\n",
       "        [17.811288 ],\n",
       "        [39.6      ],\n",
       "        [20.7108   ],\n",
       "        [15.246    ],\n",
       "        [29.1193848],\n",
       "        [16.982856 ],\n",
       "        [15.9887376],\n",
       "        [22.374    ],\n",
       "        [ 8.9884872],\n",
       "        [10.8938808],\n",
       "        [11.979    ],\n",
       "        [ 9.405    ],\n",
       "        [24.948    ],\n",
       "        [26.73     ],\n",
       "        [ 3.168    ],\n",
       "        [15.1603056],\n",
       "        [24.85296  ],\n",
       "        [ 3.663    ],\n",
       "        [31.0662   ],\n",
       "        [36.828    ],\n",
       "        [29.898    ],\n",
       "        [ 8.712    ],\n",
       "        [ 2.376    ],\n",
       "        [ 1.683    ],\n",
       "        [13.266    ],\n",
       "        [21.186    ],\n",
       "        [ 3.366    ],\n",
       "        [42.57     ],\n",
       "        [25.8056568],\n",
       "        [11.682    ],\n",
       "        [ 9.941184 ],\n",
       "        [ 7.029    ],\n",
       "        [32.076    ],\n",
       "        [17.82     ],\n",
       "        [26.532    ],\n",
       "        [37.719    ],\n",
       "        [17.0242776],\n",
       "        [33.165    ],\n",
       "        [28.413    ]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array\n",
    "\n",
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "geographic-concert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array.shape\n",
    "targets_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crucial-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "train_ds, val_ds = random_split(dataset, [225, 45])\n",
    "batch_size = 82\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "described-issue",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0212,  0.1008, -0.2690,  0.2338,  0.0352],\n",
       "         [ 0.1825, -0.2660,  0.1815,  0.3974,  0.1888],\n",
       "         [-0.2370, -0.3454,  0.2493,  0.2279, -0.1690],\n",
       "         [-0.0811,  0.3084, -0.2862,  0.2655,  0.1005],\n",
       "         [-0.0815, -0.1473,  0.4192,  0.4449, -0.3395],\n",
       "         [ 0.1071, -0.2569,  0.0414, -0.3267,  0.3823],\n",
       "         [-0.0436, -0.1430,  0.3184, -0.1105,  0.0318],\n",
       "         [-0.4449,  0.1954,  0.2193, -0.4254, -0.3688],\n",
       "         [-0.4069,  0.3332, -0.0156,  0.0090, -0.3838],\n",
       "         [ 0.0592,  0.1063,  0.0376,  0.0471,  0.3492],\n",
       "         [-0.0428, -0.3258, -0.2195,  0.0187,  0.0601],\n",
       "         [ 0.1967, -0.1560,  0.0141,  0.3141,  0.3073],\n",
       "         [ 0.1564,  0.2865,  0.0316,  0.2804,  0.3401],\n",
       "         [-0.0634, -0.2270, -0.1326,  0.1826, -0.3167],\n",
       "         [ 0.0123,  0.1516,  0.2828,  0.2004,  0.2671],\n",
       "         [-0.0173,  0.2007, -0.2070, -0.4338,  0.2179],\n",
       "         [ 0.1818, -0.1453,  0.1686,  0.2364,  0.4467],\n",
       "         [-0.1899, -0.1119, -0.1135, -0.1086,  0.1757],\n",
       "         [ 0.0672, -0.0445,  0.3936,  0.3184, -0.1304],\n",
       "         [-0.2349, -0.2198, -0.0524,  0.2236, -0.1231],\n",
       "         [-0.3801,  0.1420, -0.0765,  0.3620, -0.2051],\n",
       "         [-0.1771, -0.2112, -0.0604,  0.2778,  0.2920],\n",
       "         [ 0.0566,  0.1709,  0.1101,  0.1688,  0.4295],\n",
       "         [-0.3558,  0.3317,  0.1991, -0.3746, -0.0903],\n",
       "         [ 0.1036,  0.0238, -0.4276,  0.1846,  0.3442],\n",
       "         [-0.0169,  0.0454, -0.3162,  0.0006,  0.0269],\n",
       "         [ 0.1979, -0.4012,  0.4379, -0.1082, -0.1881],\n",
       "         [-0.2536,  0.3687, -0.0467,  0.3560, -0.0389],\n",
       "         [ 0.0805,  0.0744, -0.1156, -0.3339, -0.4246],\n",
       "         [-0.4440, -0.2288, -0.3291,  0.2253, -0.0516],\n",
       "         [-0.1701,  0.3956, -0.3149,  0.2842, -0.0477],\n",
       "         [-0.0630, -0.4237, -0.0798,  0.3895,  0.2653],\n",
       "         [ 0.2327, -0.4207, -0.2540, -0.1129, -0.2307],\n",
       "         [ 0.1231, -0.2523, -0.1633, -0.1076,  0.3606],\n",
       "         [-0.1848,  0.3571,  0.3630,  0.1965,  0.0696]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3660,  0.1181,  0.3159,  0.3048, -0.2733,  0.0998,  0.2765, -0.2870,\n",
       "         -0.3313,  0.2530,  0.0251,  0.1365,  0.3267,  0.0594, -0.1982, -0.4453,\n",
       "         -0.4224, -0.1840,  0.3563,  0.4456,  0.0682, -0.1653,  0.0538, -0.1892,\n",
       "         -0.0712, -0.0472, -0.0188,  0.2090,  0.3779,  0.0703, -0.2505, -0.3686,\n",
       "          0.1299, -0.0819,  0.2779], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0597, -0.1458, -0.0692,  ...,  0.0803, -0.1649,  0.1509],\n",
       "         [ 0.0506,  0.0445, -0.0586,  ...,  0.1046, -0.1567, -0.1321],\n",
       "         [ 0.1684,  0.1328,  0.1228,  ..., -0.0595, -0.1378,  0.0371],\n",
       "         ...,\n",
       "         [ 0.0979,  0.1188, -0.1627,  ..., -0.0745, -0.1031, -0.1545],\n",
       "         [ 0.0263,  0.0220, -0.1303,  ...,  0.0785,  0.1311, -0.1136],\n",
       "         [ 0.1540,  0.1459, -0.1373,  ...,  0.0853, -0.0085,  0.0523]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0752, -0.0200, -0.1291,  0.1660, -0.1373, -0.0960, -0.1226, -0.0326,\n",
       "         -0.1396,  0.1094, -0.1520, -0.0653, -0.1108,  0.1557,  0.1637, -0.0341,\n",
       "         -0.0423,  0.1128, -0.1496, -0.0006, -0.1530,  0.1266, -0.0089, -0.0631,\n",
       "         -0.1486, -0.1262, -0.0025,  0.0504,  0.0405, -0.0640, -0.0438,  0.1561,\n",
       "          0.1383,  0.1655, -0.1590], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1343, -0.1459,  0.1029,  0.1509,  0.0199, -0.0082,  0.0533,  0.1337,\n",
       "          -0.1554, -0.1341,  0.0413,  0.0296, -0.0729, -0.1194,  0.0249, -0.1269,\n",
       "           0.0915, -0.1524, -0.0533, -0.0426,  0.1346, -0.0719,  0.1504, -0.0064,\n",
       "          -0.0242, -0.0765, -0.0425,  0.1321,  0.1384,  0.1566,  0.0224, -0.1628,\n",
       "          -0.0106,  0.1253,  0.0020]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0272], requires_grad=True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)\n",
    "H = 35\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "class HbondModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n",
    "        self.input_linear = torch.nn.Linear(input_size, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, xb):\n",
    "#         out = self.linear(xb)                          # fill this\n",
    "#         return out\n",
    "        \n",
    "        h_relu = self.input_linear(xb).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        out = self.output_linear(h_relu)\n",
    "        return out    \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = criterion(out, targets)                         # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = criterion(out, targets)                           # fill this    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))\n",
    "    \n",
    "        \n",
    "model = HbondModel()\n",
    "\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "wrong-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 16530.931640625}\n"
     ]
    }
   ],
   "source": [
    "# Eval algorithm\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "# Fitting algorithm\n",
    "def fit(epochs, lr, momentum, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr, momentum)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "# Check the initial value that val_loss have\n",
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "theoretical-religion",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 19148.5977\n",
      "Epoch [40], val_loss: 18002.8965\n",
      "Epoch [60], val_loss: 18889.8164\n",
      "Epoch [80], val_loss: 17627.9102\n",
      "Epoch [90], val_loss: 17542.0879\n",
      "Epoch [20], val_loss: 17467.9375\n",
      "Epoch [40], val_loss: 17411.1211\n",
      "Epoch [60], val_loss: 18108.6992\n",
      "Epoch [80], val_loss: 17889.4785\n",
      "Epoch [90], val_loss: 17848.5391\n",
      "Epoch [20], val_loss: 17229.5859\n",
      "Epoch [40], val_loss: 17711.1094\n",
      "Epoch [60], val_loss: 17670.3496\n",
      "Epoch [80], val_loss: 17094.6016\n",
      "Epoch [90], val_loss: 17075.3340\n",
      "Epoch [20], val_loss: 17477.1094\n",
      "Epoch [40], val_loss: 16966.8516\n",
      "Epoch [60], val_loss: 17237.7695\n",
      "Epoch [80], val_loss: 17189.0078\n",
      "Epoch [90], val_loss: 16835.6406\n",
      "Epoch [20], val_loss: 17120.0898\n",
      "Epoch [40], val_loss: 17042.9043\n",
      "Epoch [60], val_loss: 16669.5918\n",
      "Epoch [80], val_loss: 17015.7988\n",
      "Epoch [90], val_loss: 16577.8008\n",
      "Epoch [20], val_loss: 16932.4531\n",
      "Epoch [40], val_loss: 16647.1074\n",
      "Epoch [60], val_loss: 16809.7324\n",
      "Epoch [80], val_loss: 16421.0801\n",
      "Epoch [90], val_loss: 16367.8516\n",
      "Epoch [20], val_loss: 16618.4512\n",
      "Epoch [40], val_loss: 16045.7344\n",
      "Epoch [60], val_loss: 16133.7949\n",
      "Epoch [80], val_loss: 9033.8965\n",
      "Epoch [90], val_loss: 1471.7571\n",
      "Epoch [20], val_loss: 3012.6387\n",
      "Epoch [40], val_loss: 1836.7175\n",
      "Epoch [60], val_loss: 924.8176\n",
      "Epoch [80], val_loss: 953.4807\n",
      "Epoch [90], val_loss: 1322.1476\n",
      "Epoch [20], val_loss: 922.8942\n",
      "Epoch [40], val_loss: 1231.6444\n",
      "Epoch [60], val_loss: 689.7609\n",
      "Epoch [80], val_loss: 735.4801\n",
      "Epoch [90], val_loss: 986.5301\n",
      "Epoch [20], val_loss: 890.5895\n",
      "Epoch [40], val_loss: 692.2679\n",
      "Epoch [60], val_loss: 825.4823\n",
      "Epoch [80], val_loss: 667.3058\n",
      "Epoch [90], val_loss: 790.9910\n",
      "Epoch [20], val_loss: 630.0619\n",
      "Epoch [40], val_loss: 871.9377\n",
      "Epoch [60], val_loss: 517.1940\n",
      "Epoch [80], val_loss: 787.2127\n",
      "Epoch [90], val_loss: 650.9803\n",
      "Epoch [20], val_loss: 650.2986\n",
      "Epoch [40], val_loss: 743.2869\n",
      "Epoch [60], val_loss: 797.8733\n",
      "Epoch [80], val_loss: 499.0168\n",
      "Epoch [90], val_loss: 718.1562\n",
      "Epoch [20], val_loss: 712.3122\n",
      "Epoch [40], val_loss: 545.3725\n",
      "Epoch [60], val_loss: 532.0923\n",
      "Epoch [80], val_loss: 617.8852\n",
      "Epoch [90], val_loss: 556.6935\n",
      "Epoch [20], val_loss: 723.3314\n",
      "Epoch [40], val_loss: 580.3162\n",
      "Epoch [60], val_loss: 695.7136\n",
      "Epoch [80], val_loss: 584.7299\n",
      "Epoch [90], val_loss: 630.4637\n",
      "Epoch [20], val_loss: 533.7220\n",
      "Epoch [40], val_loss: 671.0604\n",
      "Epoch [60], val_loss: 577.6451\n",
      "Epoch [80], val_loss: 552.6674\n",
      "Epoch [90], val_loss: 696.7817\n",
      "Epoch [20], val_loss: 617.5922\n",
      "Epoch [40], val_loss: 573.7458\n",
      "Epoch [60], val_loss: 584.1248\n",
      "Epoch [80], val_loss: 525.8251\n",
      "Epoch [90], val_loss: 541.5038\n",
      "Epoch [20], val_loss: 517.2766\n",
      "Epoch [40], val_loss: 558.0338\n",
      "Epoch [60], val_loss: 555.3412\n",
      "Epoch [80], val_loss: 575.2086\n",
      "Epoch [90], val_loss: 564.2648\n",
      "Epoch [20], val_loss: 627.4203\n",
      "Epoch [40], val_loss: 515.8613\n",
      "Epoch [60], val_loss: 546.7560\n",
      "Epoch [80], val_loss: 473.7353\n",
      "Epoch [90], val_loss: 567.1196\n",
      "Epoch [20], val_loss: 568.0071\n",
      "Epoch [40], val_loss: 636.0094\n",
      "Epoch [60], val_loss: 625.4513\n",
      "Epoch [80], val_loss: 552.3579\n",
      "Epoch [90], val_loss: 624.2972\n",
      "Epoch [20], val_loss: 572.6895\n",
      "Epoch [40], val_loss: 495.5883\n",
      "Epoch [60], val_loss: 630.9921\n",
      "Epoch [80], val_loss: 566.7758\n",
      "Epoch [90], val_loss: 549.7502\n",
      "Epoch [20], val_loss: 557.7859\n",
      "Epoch [40], val_loss: 607.1624\n",
      "Epoch [60], val_loss: 543.2932\n",
      "Epoch [80], val_loss: 587.4071\n",
      "Epoch [90], val_loss: 651.5511\n",
      "Epoch [20], val_loss: 596.4332\n",
      "Epoch [40], val_loss: 569.1497\n",
      "Epoch [60], val_loss: 564.8265\n",
      "Epoch [80], val_loss: 571.2922\n",
      "Epoch [90], val_loss: 577.2507\n",
      "Epoch [20], val_loss: 581.8185\n",
      "Epoch [40], val_loss: 547.8580\n",
      "Epoch [60], val_loss: 549.6216\n",
      "Epoch [80], val_loss: 602.6555\n",
      "Epoch [90], val_loss: 502.3514\n",
      "Epoch [20], val_loss: 592.6129\n",
      "Epoch [40], val_loss: 609.6146\n",
      "Epoch [60], val_loss: 586.5391\n",
      "Epoch [80], val_loss: 591.3106\n",
      "Epoch [90], val_loss: 579.0111\n",
      "Epoch [20], val_loss: 511.1392\n",
      "Epoch [40], val_loss: 547.3533\n",
      "Epoch [60], val_loss: 531.4443\n",
      "Epoch [80], val_loss: 474.5610\n",
      "Epoch [90], val_loss: 488.4460\n",
      "Epoch [20], val_loss: 516.6348\n",
      "Epoch [40], val_loss: 553.7369\n",
      "Epoch [60], val_loss: 519.2493\n",
      "Epoch [80], val_loss: 573.9360\n",
      "Epoch [90], val_loss: 540.7842\n",
      "Epoch [20], val_loss: 539.9520\n",
      "Epoch [40], val_loss: 444.3195\n",
      "Epoch [60], val_loss: 532.6949\n",
      "Epoch [80], val_loss: 595.6839\n",
      "Epoch [90], val_loss: 567.6851\n",
      "Epoch [20], val_loss: 530.6473\n",
      "Epoch [40], val_loss: 544.7020\n",
      "Epoch [60], val_loss: 523.0025\n",
      "Epoch [80], val_loss: 500.9746\n",
      "Epoch [90], val_loss: 519.0668\n",
      "Epoch [20], val_loss: 544.0610\n",
      "Epoch [40], val_loss: 558.9723\n",
      "Epoch [60], val_loss: 512.0148\n",
      "Epoch [80], val_loss: 505.9009\n",
      "Epoch [90], val_loss: 542.2578\n",
      "Epoch [20], val_loss: 554.3488\n",
      "Epoch [40], val_loss: 511.4831\n",
      "Epoch [60], val_loss: 500.4834\n",
      "Epoch [80], val_loss: 591.5606\n",
      "Epoch [90], val_loss: 581.3651\n",
      "Epoch [20], val_loss: 515.9340\n",
      "Epoch [40], val_loss: 555.1339\n",
      "Epoch [60], val_loss: 521.7673\n",
      "Epoch [80], val_loss: 591.8215\n",
      "Epoch [90], val_loss: 471.0733\n",
      "Epoch [20], val_loss: 574.2406\n",
      "Epoch [40], val_loss: 610.0441\n",
      "Epoch [60], val_loss: 524.2653\n",
      "Epoch [80], val_loss: 531.7048\n",
      "Epoch [90], val_loss: 571.9728\n",
      "Epoch [20], val_loss: 555.6076\n",
      "Epoch [40], val_loss: 574.2307\n",
      "Epoch [60], val_loss: 503.3817\n",
      "Epoch [80], val_loss: 506.7081\n",
      "Epoch [90], val_loss: 506.5790\n",
      "Epoch [20], val_loss: 493.1543\n",
      "Epoch [40], val_loss: 562.1723\n",
      "Epoch [60], val_loss: 538.1328\n",
      "Epoch [80], val_loss: 519.9965\n",
      "Epoch [90], val_loss: 539.5891\n",
      "Epoch [20], val_loss: 599.9435\n",
      "Epoch [40], val_loss: 591.5233\n",
      "Epoch [60], val_loss: 510.9599\n",
      "Epoch [80], val_loss: 533.4457\n",
      "Epoch [90], val_loss: 500.8289\n",
      "Epoch [20], val_loss: 501.2176\n",
      "Epoch [40], val_loss: 603.6348\n",
      "Epoch [60], val_loss: 460.2016\n",
      "Epoch [80], val_loss: 541.1332\n",
      "Epoch [90], val_loss: 568.6367\n",
      "Epoch [20], val_loss: 539.8569\n",
      "Epoch [40], val_loss: 573.7472\n",
      "Epoch [60], val_loss: 548.4169\n",
      "Epoch [80], val_loss: 508.2118\n",
      "Epoch [90], val_loss: 612.9097\n",
      "Epoch [20], val_loss: 485.8806\n",
      "Epoch [40], val_loss: 550.9721\n",
      "Epoch [60], val_loss: 555.0427\n",
      "Epoch [80], val_loss: 562.5494\n",
      "Epoch [90], val_loss: 559.4012\n",
      "Epoch [20], val_loss: 534.4050\n",
      "Epoch [40], val_loss: 514.1794\n",
      "Epoch [60], val_loss: 563.2808\n",
      "Epoch [80], val_loss: 544.3589\n",
      "Epoch [90], val_loss: 536.4036\n",
      "Epoch [20], val_loss: 537.1373\n",
      "Epoch [40], val_loss: 555.1934\n",
      "Epoch [60], val_loss: 550.0077\n",
      "Epoch [80], val_loss: 551.1338\n",
      "Epoch [90], val_loss: 568.8992\n",
      "Epoch [20], val_loss: 519.1055\n",
      "Epoch [40], val_loss: 583.6690\n",
      "Epoch [60], val_loss: 505.9865\n",
      "Epoch [80], val_loss: 579.8591\n",
      "Epoch [90], val_loss: 505.6660\n",
      "Epoch [20], val_loss: 501.8894\n",
      "Epoch [40], val_loss: 547.8682\n",
      "Epoch [60], val_loss: 573.0449\n",
      "Epoch [80], val_loss: 527.5300\n",
      "Epoch [90], val_loss: 511.9698\n",
      "Epoch [20], val_loss: 531.5756\n",
      "Epoch [40], val_loss: 537.0394\n",
      "Epoch [60], val_loss: 624.6249\n",
      "Epoch [80], val_loss: 561.2790\n",
      "Epoch [90], val_loss: 545.7440\n",
      "Epoch [20], val_loss: 495.2379\n",
      "Epoch [40], val_loss: 509.8010\n",
      "Epoch [60], val_loss: 510.9361\n",
      "Epoch [80], val_loss: 592.2015\n",
      "Epoch [90], val_loss: 569.2592\n",
      "Epoch [20], val_loss: 581.2522\n",
      "Epoch [40], val_loss: 510.2726\n",
      "Epoch [60], val_loss: 585.6321\n",
      "Epoch [80], val_loss: 535.0635\n",
      "Epoch [90], val_loss: 528.2045\n",
      "Epoch [20], val_loss: 564.5854\n",
      "Epoch [40], val_loss: 577.4237\n",
      "Epoch [60], val_loss: 556.1675\n",
      "Epoch [80], val_loss: 550.8291\n",
      "Epoch [90], val_loss: 508.4190\n",
      "Epoch [20], val_loss: 619.1710\n",
      "Epoch [40], val_loss: 524.3920\n",
      "Epoch [60], val_loss: 497.0720\n",
      "Epoch [80], val_loss: 542.1561\n",
      "Epoch [90], val_loss: 597.3897\n",
      "Epoch [20], val_loss: 517.7753\n",
      "Epoch [40], val_loss: 536.6445\n",
      "Epoch [60], val_loss: 570.5963\n",
      "Epoch [80], val_loss: 517.8209\n",
      "Epoch [90], val_loss: 563.3275\n",
      "Epoch [20], val_loss: 565.1713\n",
      "Epoch [40], val_loss: 518.7026\n",
      "Epoch [60], val_loss: 558.8428\n",
      "Epoch [80], val_loss: 544.3914\n",
      "Epoch [90], val_loss: 535.3633\n",
      "Epoch [20], val_loss: 524.6702\n",
      "Epoch [40], val_loss: 537.8646\n",
      "Epoch [60], val_loss: 536.8183\n",
      "Epoch [80], val_loss: 560.4482\n",
      "Epoch [90], val_loss: 557.8265\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i < 50:\n",
    "    epochs = 90\n",
    "    lr = 1e-7\n",
    "    momentum=0.3\n",
    "    history1 = fit(epochs, lr, momentum, model, train_loader, val_loader)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "hybrid-mumbai",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 550.9362\n",
      "Epoch [40], val_loss: 527.8188\n",
      "Epoch [60], val_loss: 534.9109\n",
      "Epoch [70], val_loss: 551.1903\n",
      "Epoch [20], val_loss: 533.4866\n",
      "Epoch [40], val_loss: 550.6531\n",
      "Epoch [60], val_loss: 520.8221\n",
      "Epoch [70], val_loss: 540.1868\n",
      "Epoch [20], val_loss: 546.4874\n",
      "Epoch [40], val_loss: 536.3962\n",
      "Epoch [60], val_loss: 541.8998\n",
      "Epoch [70], val_loss: 540.6271\n",
      "Epoch [20], val_loss: 554.7566\n",
      "Epoch [40], val_loss: 544.3914\n",
      "Epoch [60], val_loss: 550.2009\n",
      "Epoch [70], val_loss: 552.3621\n",
      "Epoch [20], val_loss: 544.5552\n",
      "Epoch [40], val_loss: 534.3311\n",
      "Epoch [60], val_loss: 544.7402\n",
      "Epoch [70], val_loss: 553.5935\n",
      "Epoch [20], val_loss: 539.0363\n",
      "Epoch [40], val_loss: 545.7189\n",
      "Epoch [60], val_loss: 547.4453\n",
      "Epoch [70], val_loss: 539.3395\n",
      "Epoch [20], val_loss: 534.2662\n",
      "Epoch [40], val_loss: 532.2047\n",
      "Epoch [60], val_loss: 551.8929\n",
      "Epoch [70], val_loss: 553.4098\n",
      "Epoch [20], val_loss: 556.5248\n",
      "Epoch [40], val_loss: 555.5878\n",
      "Epoch [60], val_loss: 532.7004\n",
      "Epoch [70], val_loss: 553.8215\n",
      "Epoch [20], val_loss: 538.7316\n",
      "Epoch [40], val_loss: 531.3214\n",
      "Epoch [60], val_loss: 556.1798\n",
      "Epoch [70], val_loss: 539.0875\n",
      "Epoch [20], val_loss: 556.1424\n",
      "Epoch [40], val_loss: 547.7692\n",
      "Epoch [60], val_loss: 549.1177\n",
      "Epoch [70], val_loss: 537.2786\n",
      "Epoch [20], val_loss: 546.1888\n",
      "Epoch [40], val_loss: 540.5020\n",
      "Epoch [60], val_loss: 543.2273\n",
      "Epoch [70], val_loss: 553.5272\n",
      "Epoch [20], val_loss: 533.0861\n",
      "Epoch [40], val_loss: 536.7413\n",
      "Epoch [60], val_loss: 533.7864\n",
      "Epoch [70], val_loss: 537.7993\n",
      "Epoch [20], val_loss: 532.9703\n",
      "Epoch [40], val_loss: 526.6372\n",
      "Epoch [60], val_loss: 521.6247\n",
      "Epoch [70], val_loss: 536.9955\n",
      "Epoch [20], val_loss: 544.3130\n",
      "Epoch [40], val_loss: 535.9130\n",
      "Epoch [60], val_loss: 529.3927\n",
      "Epoch [70], val_loss: 533.3451\n",
      "Epoch [20], val_loss: 552.9042\n",
      "Epoch [40], val_loss: 545.7639\n",
      "Epoch [60], val_loss: 527.8086\n",
      "Epoch [70], val_loss: 548.3671\n",
      "Epoch [20], val_loss: 553.9641\n",
      "Epoch [40], val_loss: 527.2684\n",
      "Epoch [60], val_loss: 552.7412\n",
      "Epoch [70], val_loss: 551.2957\n",
      "Epoch [20], val_loss: 547.5392\n",
      "Epoch [40], val_loss: 530.7338\n",
      "Epoch [60], val_loss: 546.6071\n",
      "Epoch [70], val_loss: 526.7391\n",
      "Epoch [20], val_loss: 541.7158\n",
      "Epoch [40], val_loss: 545.8672\n",
      "Epoch [60], val_loss: 523.6082\n",
      "Epoch [70], val_loss: 547.2969\n",
      "Epoch [20], val_loss: 538.6598\n",
      "Epoch [40], val_loss: 535.4049\n",
      "Epoch [60], val_loss: 534.1915\n",
      "Epoch [70], val_loss: 528.9950\n",
      "Epoch [20], val_loss: 557.6343\n",
      "Epoch [40], val_loss: 558.3010\n",
      "Epoch [60], val_loss: 527.8292\n",
      "Epoch [70], val_loss: 530.6087\n",
      "Epoch [20], val_loss: 546.8701\n",
      "Epoch [40], val_loss: 543.0349\n",
      "Epoch [60], val_loss: 536.3191\n",
      "Epoch [70], val_loss: 536.7982\n",
      "Epoch [20], val_loss: 537.5284\n",
      "Epoch [40], val_loss: 551.9161\n",
      "Epoch [60], val_loss: 543.4294\n",
      "Epoch [70], val_loss: 536.8389\n",
      "Epoch [20], val_loss: 542.6335\n",
      "Epoch [40], val_loss: 537.9015\n",
      "Epoch [60], val_loss: 541.5275\n",
      "Epoch [70], val_loss: 528.1771\n",
      "Epoch [20], val_loss: 561.8495\n",
      "Epoch [40], val_loss: 547.0940\n",
      "Epoch [60], val_loss: 541.1309\n",
      "Epoch [70], val_loss: 540.5439\n",
      "Epoch [20], val_loss: 545.3431\n",
      "Epoch [40], val_loss: 540.6482\n",
      "Epoch [60], val_loss: 539.1376\n",
      "Epoch [70], val_loss: 554.3613\n",
      "Epoch [20], val_loss: 537.1995\n",
      "Epoch [40], val_loss: 540.5787\n",
      "Epoch [60], val_loss: 536.1427\n",
      "Epoch [70], val_loss: 541.4703\n",
      "Epoch [20], val_loss: 549.8901\n",
      "Epoch [40], val_loss: 539.9380\n",
      "Epoch [60], val_loss: 553.0668\n",
      "Epoch [70], val_loss: 529.6521\n",
      "Epoch [20], val_loss: 542.4470\n",
      "Epoch [40], val_loss: 538.5557\n",
      "Epoch [60], val_loss: 537.5429\n",
      "Epoch [70], val_loss: 530.6538\n",
      "Epoch [20], val_loss: 557.9865\n",
      "Epoch [40], val_loss: 537.5878\n",
      "Epoch [60], val_loss: 539.3819\n",
      "Epoch [70], val_loss: 525.8843\n",
      "Epoch [20], val_loss: 526.6817\n",
      "Epoch [40], val_loss: 555.0804\n",
      "Epoch [60], val_loss: 545.7551\n",
      "Epoch [70], val_loss: 525.5203\n",
      "Epoch [20], val_loss: 534.6714\n",
      "Epoch [40], val_loss: 558.6967\n",
      "Epoch [60], val_loss: 538.0070\n",
      "Epoch [70], val_loss: 540.1435\n",
      "Epoch [20], val_loss: 550.4871\n",
      "Epoch [40], val_loss: 552.5229\n",
      "Epoch [60], val_loss: 551.5519\n",
      "Epoch [70], val_loss: 568.6614\n",
      "Epoch [20], val_loss: 536.9534\n",
      "Epoch [40], val_loss: 534.7765\n",
      "Epoch [60], val_loss: 538.8284\n",
      "Epoch [70], val_loss: 537.8752\n",
      "Epoch [20], val_loss: 553.4802\n",
      "Epoch [40], val_loss: 545.6125\n",
      "Epoch [60], val_loss: 535.8073\n",
      "Epoch [70], val_loss: 527.7511\n",
      "Epoch [20], val_loss: 532.4888\n",
      "Epoch [40], val_loss: 549.9108\n",
      "Epoch [60], val_loss: 553.4009\n",
      "Epoch [70], val_loss: 537.3712\n",
      "Epoch [20], val_loss: 539.9965\n",
      "Epoch [40], val_loss: 552.7855\n",
      "Epoch [60], val_loss: 543.2874\n",
      "Epoch [70], val_loss: 540.1544\n",
      "Epoch [20], val_loss: 546.7390\n",
      "Epoch [40], val_loss: 540.1254\n",
      "Epoch [60], val_loss: 556.2036\n",
      "Epoch [70], val_loss: 551.5095\n",
      "Epoch [20], val_loss: 558.4466\n",
      "Epoch [40], val_loss: 534.9095\n",
      "Epoch [60], val_loss: 532.3373\n",
      "Epoch [70], val_loss: 531.5734\n",
      "Epoch [20], val_loss: 534.4855\n",
      "Epoch [40], val_loss: 526.7390\n",
      "Epoch [60], val_loss: 539.2965\n",
      "Epoch [70], val_loss: 553.0131\n",
      "Epoch [20], val_loss: 541.9719\n",
      "Epoch [40], val_loss: 537.5720\n",
      "Epoch [60], val_loss: 536.3864\n",
      "Epoch [70], val_loss: 539.7115\n",
      "Epoch [20], val_loss: 554.7604\n",
      "Epoch [40], val_loss: 535.9227\n",
      "Epoch [60], val_loss: 543.6741\n",
      "Epoch [70], val_loss: 550.1527\n",
      "Epoch [20], val_loss: 535.6882\n",
      "Epoch [40], val_loss: 542.5608\n",
      "Epoch [60], val_loss: 536.9902\n",
      "Epoch [70], val_loss: 536.4340\n",
      "Epoch [20], val_loss: 556.6104\n",
      "Epoch [40], val_loss: 532.1588\n",
      "Epoch [60], val_loss: 553.9578\n",
      "Epoch [70], val_loss: 546.1059\n",
      "Epoch [20], val_loss: 536.4894\n",
      "Epoch [40], val_loss: 541.3931\n",
      "Epoch [60], val_loss: 546.3327\n",
      "Epoch [70], val_loss: 559.8862\n",
      "Epoch [20], val_loss: 558.5020\n",
      "Epoch [40], val_loss: 536.8591\n",
      "Epoch [60], val_loss: 552.0145\n",
      "Epoch [70], val_loss: 531.6965\n",
      "Epoch [20], val_loss: 544.9210\n",
      "Epoch [40], val_loss: 552.6138\n",
      "Epoch [60], val_loss: 532.5728\n",
      "Epoch [70], val_loss: 553.5529\n",
      "Epoch [20], val_loss: 537.5640\n",
      "Epoch [40], val_loss: 542.1432\n",
      "Epoch [60], val_loss: 555.8809\n",
      "Epoch [70], val_loss: 540.8256\n",
      "Epoch [20], val_loss: 523.3185\n",
      "Epoch [40], val_loss: 546.4268\n",
      "Epoch [60], val_loss: 534.1646\n",
      "Epoch [70], val_loss: 525.5575\n",
      "Epoch [20], val_loss: 525.5305\n",
      "Epoch [40], val_loss: 527.9297\n",
      "Epoch [60], val_loss: 555.0819\n",
      "Epoch [70], val_loss: 525.1619\n",
      "Epoch [20], val_loss: 543.2141\n",
      "Epoch [40], val_loss: 553.6849\n",
      "Epoch [60], val_loss: 552.8255\n",
      "Epoch [70], val_loss: 540.4904\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i < 50:\n",
    "    epochs = 70\n",
    "    lr = 1e-8\n",
    "    momentum = 0.2\n",
    "    history1 = fit(epochs, lr, momentum, model, train_loader, val_loader)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "discrete-ceramic",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 552.8337\n",
      "Epoch [20], val_loss: 552.1411\n",
      "Epoch [20], val_loss: 532.4872\n",
      "Epoch [20], val_loss: 537.6550\n",
      "Epoch [20], val_loss: 537.4148\n",
      "Epoch [20], val_loss: 542.1710\n",
      "Epoch [20], val_loss: 536.8849\n",
      "Epoch [20], val_loss: 541.7377\n",
      "Epoch [20], val_loss: 541.9419\n",
      "Epoch [20], val_loss: 552.3259\n",
      "Epoch [20], val_loss: 537.7147\n",
      "Epoch [20], val_loss: 552.2984\n",
      "Epoch [20], val_loss: 535.3956\n",
      "Epoch [20], val_loss: 551.9797\n",
      "Epoch [20], val_loss: 537.5587\n",
      "Epoch [20], val_loss: 537.3123\n",
      "Epoch [20], val_loss: 537.7343\n",
      "Epoch [20], val_loss: 533.3666\n",
      "Epoch [20], val_loss: 541.1398\n",
      "Epoch [20], val_loss: 533.1600\n",
      "Epoch [20], val_loss: 541.4334\n",
      "Epoch [20], val_loss: 541.3278\n",
      "Epoch [20], val_loss: 532.5068\n",
      "Epoch [20], val_loss: 552.0036\n",
      "Epoch [20], val_loss: 542.3297\n",
      "Epoch [20], val_loss: 540.8909\n",
      "Epoch [20], val_loss: 551.2909\n",
      "Epoch [20], val_loss: 539.3104\n",
      "Epoch [20], val_loss: 544.9047\n",
      "Epoch [20], val_loss: 552.4296\n",
      "Epoch [20], val_loss: 540.1358\n",
      "Epoch [20], val_loss: 531.0533\n",
      "Epoch [20], val_loss: 529.3444\n",
      "Epoch [20], val_loss: 551.3641\n",
      "Epoch [20], val_loss: 551.6195\n",
      "Epoch [20], val_loss: 551.7659\n",
      "Epoch [20], val_loss: 545.4941\n",
      "Epoch [20], val_loss: 530.3634\n",
      "Epoch [20], val_loss: 545.9513\n",
      "Epoch [20], val_loss: 551.5199\n",
      "Epoch [20], val_loss: 551.6643\n",
      "Epoch [20], val_loss: 546.1570\n",
      "Epoch [20], val_loss: 539.9316\n",
      "Epoch [20], val_loss: 539.8032\n",
      "Epoch [20], val_loss: 540.4917\n",
      "Epoch [20], val_loss: 540.5186\n",
      "Epoch [20], val_loss: 552.6058\n",
      "Epoch [20], val_loss: 543.0311\n",
      "Epoch [20], val_loss: 530.7870\n",
      "Epoch [20], val_loss: 540.0831\n",
      "Epoch [20], val_loss: 551.6562\n",
      "Epoch [20], val_loss: 530.4297\n",
      "Epoch [20], val_loss: 545.7426\n",
      "Epoch [20], val_loss: 544.2596\n",
      "Epoch [20], val_loss: 530.8466\n",
      "Epoch [20], val_loss: 552.5402\n",
      "Epoch [20], val_loss: 542.1147\n",
      "Epoch [20], val_loss: 530.5643\n",
      "Epoch [20], val_loss: 530.0348\n",
      "Epoch [20], val_loss: 543.9187\n",
      "Epoch [20], val_loss: 528.8782\n",
      "Epoch [20], val_loss: 552.7783\n",
      "Epoch [20], val_loss: 541.4543\n",
      "Epoch [20], val_loss: 530.3871\n",
      "Epoch [20], val_loss: 540.6926\n",
      "Epoch [20], val_loss: 529.9297\n",
      "Epoch [20], val_loss: 540.0759\n",
      "Epoch [20], val_loss: 553.0723\n",
      "Epoch [20], val_loss: 553.3409\n",
      "Epoch [20], val_loss: 542.7410\n",
      "Epoch [20], val_loss: 532.0494\n",
      "Epoch [20], val_loss: 553.9312\n",
      "Epoch [20], val_loss: 539.7142\n",
      "Epoch [20], val_loss: 539.5050\n",
      "Epoch [20], val_loss: 543.5729\n",
      "Epoch [20], val_loss: 531.2379\n",
      "Epoch [20], val_loss: 530.9873\n",
      "Epoch [20], val_loss: 543.5838\n",
      "Epoch [20], val_loss: 545.5500\n",
      "Epoch [20], val_loss: 553.0366\n",
      "Epoch [20], val_loss: 546.1859\n",
      "Epoch [20], val_loss: 552.7852\n",
      "Epoch [20], val_loss: 544.8997\n",
      "Epoch [20], val_loss: 553.0124\n",
      "Epoch [20], val_loss: 543.3511\n",
      "Epoch [20], val_loss: 542.1834\n",
      "Epoch [20], val_loss: 554.3071\n",
      "Epoch [20], val_loss: 531.4187\n",
      "Epoch [20], val_loss: 538.9500\n",
      "Epoch [20], val_loss: 553.9092\n",
      "Epoch [20], val_loss: 531.8842\n",
      "Epoch [20], val_loss: 531.7532\n",
      "Epoch [20], val_loss: 540.4763\n",
      "Epoch [20], val_loss: 541.8331\n",
      "Epoch [20], val_loss: 554.1118\n",
      "Epoch [20], val_loss: 531.1245\n",
      "Epoch [20], val_loss: 531.1395\n",
      "Epoch [20], val_loss: 539.8085\n",
      "Epoch [20], val_loss: 530.9097\n",
      "Epoch [20], val_loss: 543.4609\n"
     ]
    }
   ],
   "source": [
    "# Train repeatdly until have a 'good' val_loss\n",
    "i=0\n",
    "while i < 100:\n",
    "    epochs = 20\n",
    "    lr = 1e-9\n",
    "    momentum = 0.1\n",
    "    history1 = fit(epochs, lr, momentum,  model, train_loader, val_loader)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "present-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([  2.4800, 177.0000,   2.9000, 166.0000,   1.1600])\n",
      "Target: tensor([1.5840])\n",
      "Prediction: tensor([3.0776])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.0776])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction Algorithm\n",
    "def validation(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"Input:\", input)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"Prediction:\", prediction)\n",
    "    return prediction\n",
    "\n",
    "# Testing the model with some samples\n",
    "input, target = val_ds[0]\n",
    "validation(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "stock-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Algorithm\n",
    "def predict(values, model):\n",
    "    inputs = values.unsqueeze(0)\n",
    "    predictions = model(inputs)\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"The input values are:\", inputs)\n",
    "    print(\"The H bond energy is:\", prediction, 'kJ/mol')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "popular-stone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input values are: tensor([[  2.9000, 166.0000,   2.7400, 204.0000, 145.0000]])\n",
      "The H bond energy is: tensor([14.0984]) kJ/mol\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([14.0984])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.tensor([2.9, 166, 2.74, 204, 145])\n",
    "predict(values,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bridal-inflation",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[2.4800e+00, 1.7700e+02, 2.9000e+00, 1.6600e+02, 1.1600e+00],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 3.7494e+02],\n",
      "        [2.4800e+00, 1.7700e+02, 3.4100e+00, 1.5000e+02, 1.7400e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 4.6292e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 3.8734e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.0400e+00, 1.7700e+02, 9.4188e+01],\n",
      "        [2.4800e+00, 1.7700e+02, 3.4100e+00, 1.5000e+02, 3.8280e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 1.3693e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 1.9024e+02],\n",
      "        [2.4800e+00, 1.7700e+02, 3.4100e+00, 1.5000e+02, 6.8440e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 3.5844e+02],\n",
      "        [2.4800e+00, 1.7700e+02, 3.4100e+00, 1.5000e+02, 1.8792e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 1.9720e+03],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 3.5026e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 3.6076e+02],\n",
      "        [2.6900e+00, 1.8900e+02, 2.9000e+00, 1.6600e+02, 1.6240e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 1.2639e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 1.1774e+03],\n",
      "        [2.6900e+00, 1.8900e+02, 2.3600e+00, 1.7700e+02, 2.3200e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 2.4940e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 2.6216e+02],\n",
      "        [2.9000e+00, 1.6600e+02, 3.4100e+00, 1.5000e+02, 6.6816e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 6.0287e+02],\n",
      "        [2.9000e+00, 1.6600e+02, 2.9000e+00, 1.6600e+02, 2.8378e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 2.7983e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 6.4148e+02],\n",
      "        [2.4800e+00, 1.7700e+02, 2.9000e+00, 1.6600e+02, 4.6400e+00],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 4.3616e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 5.7072e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 2.8031e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 3.0972e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 3.2091e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 2.3066e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.0400e+00, 1.7700e+02, 9.2800e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 8.9145e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 2.8072e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 3.2019e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 6.3800e+01],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 1.0788e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 9.5120e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 7.3080e+01],\n",
      "        [3.1000e+00, 1.8200e+02, 3.4100e+00, 1.5000e+02, 4.6400e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 4.6292e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 3.4100e+00, 1.5000e+02, 4.4312e+02],\n",
      "        [3.4100e+00, 1.5000e+02, 2.9000e+00, 1.6600e+02, 1.1716e+02]])\n",
      "Target: tensor([[ 1.5840],\n",
      "        [21.5392],\n",
      "        [ 4.6530],\n",
      "        [19.4682],\n",
      "        [22.3677],\n",
      "        [ 7.9529],\n",
      "        [ 4.7520],\n",
      "        [12.1780],\n",
      "        [12.8700],\n",
      "        [ 5.8410],\n",
      "        [27.7200],\n",
      "        [12.0780],\n",
      "        [45.5638],\n",
      "        [17.0243],\n",
      "        [17.4240],\n",
      "        [ 9.9000],\n",
      "        [14.3319],\n",
      "        [44.5500],\n",
      "        [ 2.0790],\n",
      "        [18.2160],\n",
      "        [19.3050],\n",
      "        [32.0760],\n",
      "        [28.6223],\n",
      "        [20.7108],\n",
      "        [12.4679],\n",
      "        [28.1160],\n",
      "        [ 3.1680],\n",
      "        [20.1309],\n",
      "        [31.0662],\n",
      "        [17.1485],\n",
      "        [15.7410],\n",
      "        [17.3971],\n",
      "        [10.1897],\n",
      "        [ 7.9200],\n",
      "        [11.0596],\n",
      "        [12.8700],\n",
      "        [15.1603],\n",
      "        [ 9.4050],\n",
      "        [ 9.1080],\n",
      "        [37.7190],\n",
      "        [ 8.1180],\n",
      "        [23.1660],\n",
      "        [24.8530],\n",
      "        [23.7600],\n",
      "        [ 7.0290]])\n",
      "Prediction: tensor([[ 3.6708],\n",
      "        [22.1694],\n",
      "        [ 4.2373],\n",
      "        [26.1511],\n",
      "        [22.6712],\n",
      "        [10.2387],\n",
      "        [ 5.1625],\n",
      "        [12.2790],\n",
      "        [14.4973],\n",
      "        [ 7.4347],\n",
      "        [21.9006],\n",
      "        [14.7040],\n",
      "        [55.5227],\n",
      "        [21.5598],\n",
      "        [21.9974],\n",
      "        [14.1797],\n",
      "        [11.8402],\n",
      "        [39.5391],\n",
      "        [ 4.7591],\n",
      "        [17.3629],\n",
      "        [17.4899],\n",
      "        [30.7521],\n",
      "        [30.2444],\n",
      "        [18.9732],\n",
      "        [18.6290],\n",
      "        [29.7095],\n",
      "        [ 3.8181],\n",
      "        [25.0686],\n",
      "        [29.6548],\n",
      "        [18.6491],\n",
      "        [19.4690],\n",
      "        [20.3383],\n",
      "        [16.5832],\n",
      "        [10.1078],\n",
      "        [ 9.6770],\n",
      "        [18.6661],\n",
      "        [20.3087],\n",
      "        [ 7.2589],\n",
      "        [11.0272],\n",
      "        [36.6333],\n",
      "        [ 8.1559],\n",
      "        [26.1865],\n",
      "        [26.1511],\n",
      "        [24.9276],\n",
      "        [11.7992]])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[:]\n",
    "predictions = validation(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "average-coast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 164.16x145.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.linspace(0,53,20)\n",
    "b = a\n",
    "fig = plt.figure(figsize=(2.28,2.025))\n",
    "ax = fig.add_axes([0.18, 0.21, 0.76, 0.76])\n",
    "ax.scatter(predictions,target, c='tab:blue', s =26, alpha = 0.6, edgecolors='none')\n",
    "ax.plot(a,b, color='tab:red', alpha = 0.6)\n",
    "ax.tick_params(axis='x', pad=1.2, labelsize= 8)\n",
    "ax.tick_params(axis='y', pad=1.2, labelsize= 8)\n",
    "ax.set_xlabel('Predictions', fontsize=8)\n",
    "ax.set_ylabel('Actual values', fontsize=8)\n",
    "plt.savefig('modeleva.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "contrary-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = './HbondModel.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "seventh-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HbondModel(\n",
       "  (input_linear): Linear(in_features=5, out_features=35, bias=True)\n",
       "  (middle_linear): Linear(in_features=35, out_features=35, bias=True)\n",
       "  (output_linear): Linear(in_features=35, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "hmodel = HbondModel()\n",
    "hmodel.load_state_dict(torch.load(save_path))\n",
    "hmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "subjective-contrary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input values are: tensor([[  2.9000, 166.0000,   2.7400, 204.0000, 145.0000]])\n",
      "The H bond energy is: tensor([13.8176]) kJ/mol\n"
     ]
    }
   ],
   "source": [
    "values = torch.tensor([2.9, 166, 2.74, 204, 145])\n",
    "mapipredict = predict(values, hmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
